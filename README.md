# Music Listener Behavior Analysis Using Spark Structured API

## Overview
This project analyzes music listener behavior and music trends for a fictional music streaming service. It uses two datasets:
- *listening_logs.csv*: Contains logs of user song plays with the following columns:
  - user_id: Unique identifier for each user.
  - song_id: Unique identifier for each song.
  - timestamp: Date and time the song was played.
  - duration_sec: The duration (in seconds) for which the song was played.
- *songs_metadata.csv*: Contains metadata for each song with the following columns:
  - song_id: Unique identifier for each song.
  - title: Title of the song.
  - artist: Artist’s name.
  - genre: The genre of the song (e.g., Pop, Rock, Jazz, Hip-Hop, Classical).
  - mood: The mood of the song (e.g., Happy, Sad, Energetic, Chill).

The Spark analysis script (code1.py) performs these tasks:
1. *Favorite Genre per User*: Determines each user’s favorite genre by counting song plays.
2. *Average Listen Time per Song*: Computes the average playing duration per song.
3. *Top 10 Most Played Songs This Week*: Filters the data using the current week and outputs the top 10 songs by play count.
4. *Happy Song Recommendations*: Identifies users who mostly listen to "Sad" songs and recommends up to 3 "Happy" songs they haven't played.
5. *Genre Loyalty Score*: Calculates each user's genre loyalty (the ratio of plays from their favorite genre) and outputs users with a score above 0.8.
6. *Night Owl Users*: Identifies users who listen to music between 12 AM and 5 AM.

## Project Files
- *generate_data.py*: Generates random data for songs_metadata.csv and listening_logs.csv.  
  This script also biases the data so that some users have a higher proportion of “Sad” plays and some have high genre loyalty, ensuring non-empty outputs for the analysis tasks.
- *code1.py*: The PySpark analysis script that reads the CSV files, processes the data, and writes results to an output directory.
- *songs_metadata.csv*: Generated by generate_data.py.
- *listening_logs.csv*: Generated by generate_data.py.
- *Docker Files*: A docker-compose.yml file (not shown here) is used to launch the Spark container (assumed to be named my-spark-master).

## Steps to Run the Project
Follow these steps to run the entire workflow:

### 1. Start the Docker Containers
Ensure Docker is installed and the docker-compose.yml is in the project root. Then run:
bash
docker-compose up -d


### 2. Generate the Data Files
Run the data generation script to create the CSV files:
bash
python .\generate_data.py


### 3. Copy Files into the Spark Container
Copy the analysis script and the generated CSV files into your Spark container:
bash
docker cp .\code1.py my-spark-master:/opt/bitnami/spark
docker cp .\songs_metadata.csv my-spark-master:/opt/bitnami/spark
docker cp .\listening_logs.csv my-spark-master:/opt/bitnami/spark


### 4. Run the Spark Analysis Job
Enter the Spark container shell:
bash
docker exec -it my-spark-master bash


Inside the container, run the Spark job using:

bash
spark-submit code1.py


5. Retrieve the Output Files
After the job completes, exit the container shell and copy the output folder from the container to your host machine:
bash
exit
docker cp my-spark-master:/opt/bitnami/spark/output/ /outputs
